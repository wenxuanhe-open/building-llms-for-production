{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Set Proxy ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Proxy: http://127.0.0.1:7890\n",
      "HTTPS Proxy: http://127.0.0.1:7890\n",
      "All Proxy: socks5://127.0.0.1:7890\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 设置 HTTP 代理\n",
    "os.environ['http_proxy'] = \"http://127.0.0.1:7890\"\n",
    "os.environ['https_proxy'] = \"http://127.0.0.1:7890\"\n",
    "os.environ['all_proxy'] = \"socks5://127.0.0.1:7890\"\n",
    "\n",
    "# 确认代理已设置\n",
    "print(\"HTTP Proxy:\", os.environ.get('http_proxy'))\n",
    "print(\"HTTPS Proxy:\", os.environ.get('https_proxy'))\n",
    "print(\"All Proxy:\", os.environ.get('all_proxy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Architecture In Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Installation\\Anaconda\\envs\\llmc\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp_tokenized['input_ids'].size(): torch.Size([1, 10])\n",
      "inp_tokenized: {'input_ids': tensor([[    2,   133,  2119,  6219, 23602, 13855,    81,     5, 22414,  2335]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Configure quantization with BitsAndBytesConfig for 8-bit loading  gpu \n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "# Load the pre-trained model and tokenizer from the Hugging Face model hub\n",
    "OPT = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", quantization_config=quantization_config)  # Load model in 8-bit mode to reduce memory usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")  # Load tokenizer for the same model\n",
    "\n",
    "# Define a sample input sentence\n",
    "inp = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Tokenize the input sentence and return tensors in PyTorch format\n",
    "inp_tokenized = tokenizer(inp, return_tensors=\"pt\") \n",
    "\n",
    "# Print the size of the tokenized input tensor (shows the dimensions)\n",
    "print(\"inp_tokenized['input_ids'].size():\", inp_tokenized['input_ids'].size())  \n",
    "\n",
    "# Print the tokenized input (shows the token IDs corresponding to each word in the sentence)\n",
    "print(\"inp_tokenized:\", inp_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTModel(\n",
      "  (decoder): OPTDecoder(\n",
      "    (embed_tokens): Embedding(50272, 2048, padding_idx=1)\n",
      "    (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
      "    (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x OPTDecoderLayer(\n",
      "        (self_attn): OPTSdpaAttention(\n",
      "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "        )\n",
      "        (activation_fn): ReLU()\n",
      "        (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "        (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# examine the model’s architecture\n",
    "print(OPT.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer:\t Embedding(50272, 2048, padding_idx=1)\n",
      "Size:\t torch.Size([1, 10, 2048])\n",
      "Output:\t tensor([[[-0.0407,  0.0519,  0.0574,  ..., -0.0263, -0.0355, -0.0260],\n",
      "         [-0.0371,  0.0220, -0.0096,  ...,  0.0265, -0.0166, -0.0030],\n",
      "         [-0.0455, -0.0236, -0.0121,  ...,  0.0043, -0.0166,  0.0193],\n",
      "         ...,\n",
      "         [ 0.0007,  0.0267,  0.0257,  ...,  0.0622,  0.0421,  0.0279],\n",
      "         [-0.0126,  0.0347, -0.0352,  ..., -0.0393, -0.0396, -0.0102],\n",
      "         [-0.0115,  0.0319,  0.0274,  ..., -0.0472, -0.0059,  0.0341]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# examine the Input Embedding\n",
    "embedded_input = OPT.model.decoder.embed_tokens(inp_tokenized['input_ids'])\n",
    "\n",
    "print(\"Layer:\\t\", OPT.model.decoder.embed_tokens)\n",
    "print(\"Size:\\t\", embedded_input.size())\n",
    "print(\"Output:\\t\", embedded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer:\t OPTLearnedPositionalEmbedding(2050, 2048)\n",
      "Size:\t torch.Size([1, 10, 2048])\n",
      "Output:\t tensor([[[-8.1406e-03, -2.6221e-01,  6.0768e-03,  ...,  1.7273e-02,\n",
      "          -5.0621e-03, -1.6220e-02],\n",
      "         [-8.0585e-05,  2.5000e-01, -1.6632e-02,  ..., -1.5419e-02,\n",
      "          -1.7838e-02,  2.4948e-02],\n",
      "         [-9.9411e-03, -1.4978e-01,  1.7557e-03,  ...,  3.7117e-03,\n",
      "          -1.6434e-02, -9.9087e-04],\n",
      "         ...,\n",
      "         [ 3.6979e-04, -7.7454e-02,  1.2955e-02,  ...,  3.9330e-03,\n",
      "          -1.1642e-02,  7.8506e-03],\n",
      "         [-2.6779e-03, -2.2446e-02, -1.6754e-02,  ..., -1.3142e-03,\n",
      "          -7.8583e-03,  2.0096e-02],\n",
      "         [-8.6288e-03,  1.4233e-01, -1.9012e-02,  ..., -1.8463e-02,\n",
      "          -9.8572e-03,  8.7662e-03]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# examine the Positional Encoding\n",
    "embed_pos_input = OPT.model.decoder.embed_positions(inp_tokenized['attention_mask'])\n",
    "\n",
    "print(\"Layer:\\t\", OPT.model.decoder.embed_positions)\n",
    "print(\"Size:\\t\", embed_pos_input.size())\n",
    "print(\"Output:\\t\", embed_pos_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer:\t OPTSdpaAttention(\n",
      "  (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      ")\n",
      "Size:\t torch.Size([1, 10, 2048])\n",
      "Output:\t tensor([[[-0.0135, -0.0095,  0.0013,  ...,  0.0065, -0.0017,  0.0134],\n",
      "         [-0.0130, -0.0102,  0.0022,  ...,  0.0087,  0.0004,  0.0124],\n",
      "         [-0.0131, -0.0059,  0.0038,  ...,  0.0098,  0.0020,  0.0141],\n",
      "         ...,\n",
      "         [-0.0121, -0.0098,  0.0050,  ...,  0.0095,  0.0015,  0.0098],\n",
      "         [-0.0119, -0.0101,  0.0051,  ...,  0.0094,  0.0011,  0.0091],\n",
      "         [-0.0118, -0.0109,  0.0055,  ...,  0.0095,  0.0013,  0.0091]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# examine the first layer’s selfattention component \n",
    "# Add token embeddings and positional embeddings\n",
    "embed_position_input = embedded_input + embed_pos_input\n",
    "# Pass the input through the self-attention mechanism of layer 0\n",
    "hidden_states, _, _ = OPT.model.decoder.layers[0].self_attn(embed_position_input)\n",
    "print(\"Layer:\\t\", OPT.model.decoder.layers[0].self_attn)\n",
    "print(\"Size:\\t\", hidden_states.size())\n",
    "print(\"Output:\\t\", hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder-Decoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartModel(\n",
      "  (shared): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
      "  (encoder): BartEncoder(\n",
      "    (embed_tokens): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
      "    (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x BartEncoderLayer(\n",
      "        (self_attn): BartSdpaAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation_fn): GELUActivation()\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): BartDecoder(\n",
      "    (embed_tokens): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
      "    (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x BartDecoderLayer(\n",
      "        (self_attn): BartSdpaAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_fn): GELUActivation()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): BartSdpaAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# load the BART model\n",
    "BART = AutoModel.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "# examine the model’s architecture\n",
    "print(BART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bennett and Gaga became fast friends and close collaborators. They recorded two albums together, 2014's \"Cheek to Cheek\" and 2021's \"Love for Sale\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU) is available, and set device accordingly\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=device)\n",
    "sum = summarizer(\"\"\"Gaga was best known in the 2010s for pop hits like “Poker Face” and avant-garde experimentation on albums like “Artpop,” and Bennett, a singer who mostly stuck to standards, was in his 80s when the pair met. And yet Bennett and Gaga became fast friends and close collaborators, which they remained until Bennett’s death at 96 on Friday. They recorded two albums together, 2014’s “Cheek to Cheek” and 2021’s “Love for Sale,” which both won Grammys for best traditional pop vocal album.\"\"\", min_length=20, max_length=50)\n",
    "\n",
    "print(sum[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder-Only Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# load the BERT model  \n",
    "BERT = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# examine the model’s architecture\n",
    "print(BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder-Only Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# load the gpt2 model\n",
    "GPT2 = AutoModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# examine the model’s architecture\n",
    "print(GPT2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">  personal project for me, but then it's really a film I love.\n",
      "\n",
      "How did The Girl with the Dragon Tattoo come to be?\n",
      "\n",
      "I knew my name was coming in, and just then I started to develop that identity through\n",
      ">  short one, but it's got a lot to offer and I think it's a great idea to do it in a more theatrical manner. The world looks more realistic than if you've seen some of the more recent films.\"\n",
      "\n",
      "The film is\n",
      ">  high profile and very high profile hit and I'm really looking forward to making it again, I am a huge fan.\n",
      "\n",
      "Logan: So to find out who the cast of The Muppets are, you get that kind of an amazing\n",
      ">  successful and well received reboot of the series. The original movie was a successful and well received adaptation. A sequel was written. There was a change in direction of the series and was released in 1977. Then the original was made into a movie and was\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU) is available, and set device accordingly\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# Initialize a text generation pipeline using the GPT-2 model\n",
    "generator = pipeline(model=\"gpt2\",device=device)\n",
    "\n",
    "# Generate text based on the input prompt\n",
    "output = generator(\n",
    "    \"This movie was a very\",\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=4,\n",
    "    max_new_tokens=50,\n",
    "    return_full_text=False\n",
    ")\n",
    "\n",
    "# Print each generated text sequence\n",
    "for item in output:\n",
    "    print(\">\", item['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      " [[0.37454012 0.95071431 0.73199394 0.59865848]]\n",
      "Key:\n",
      " [[0.15601864 0.15599452 0.05808361 0.86617615]\n",
      " [0.60111501 0.70807258 0.02058449 0.96990985]\n",
      " [0.83244264 0.21233911 0.18182497 0.18340451]]\n",
      "Value:\n",
      " [[0.30424224 0.52475643 0.43194502 0.29122914]\n",
      " [0.61185289 0.13949386 0.29214465 0.36636184]\n",
      " [0.45606998 0.78517596 0.19967378 0.51423444]]\n",
      "Mask:\n",
      " [[1 1 0]]\n",
      "Attention Output:\n",
      " [[0.45606998 0.78517596 0.19967378 0.51423444]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def self_attention(query, key, value, mask=None):\n",
    "    scores = np.dot(query, key.T)\n",
    "\n",
    "    if mask is not None:\n",
    "        # Set masked positions to a very large negative value to exclude them\n",
    "        scores = scores + (mask * -1e9)\n",
    "\n",
    "    # Apply softmax to obtain attention weights\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n",
    "\n",
    "    # Compute weighted sum of value vectors\n",
    "    output = np.dot(attention_weights, value)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Example data for query, key, and value\n",
    "np.random.seed(42)  # For reproducibility\n",
    "query = np.random.rand(1, 4)  # One query vector with depth 4\n",
    "key = np.random.rand(3, 4)    # Three key vectors with depth 4\n",
    "value = np.random.rand(3, 4)  # Three value vectors with depth 4\n",
    "\n",
    "# Binary mask (1 for valid positions, 0 for masked positions)\n",
    "mask = np.array([[1, 1, 0]])  # Mask the last key by setting it to 0\n",
    "\n",
    "# Perform self-attention\n",
    "output = self_attention(query, key, value, mask=mask)\n",
    "\n",
    "# Display results\n",
    "print(\"Query:\\n\", query)\n",
    "print(\"Key:\\n\", key)\n",
    "print(\"Value:\\n\", value)\n",
    "print(\"Mask:\\n\", mask)\n",
    "print(\"Attention Output:\\n\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cohere LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isaac Newton was born on 25 December 1642, according to the Julian calendar in use in England at the time.\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "\n",
    "# Initialize the Cohere client (API key is omitted and will be read from the environment variable)\n",
    "co = cohere.Client()\n",
    "\n",
    "# Set up chat history and the question\n",
    "response = co.chat(\n",
    "    chat_history=[\n",
    "        {\"role\": \"USER\", \"message\": \"Who discovered gravity?\"},\n",
    "        {\"role\": \"CHATBOT\", \"message\": \"The man who is widely credited with discovering gravity is Sir Isaac Newton\"}\n",
    "    ],\n",
    "    message=\"What year was he born?\",  \n",
    "    connectors=[{\"id\": \"web-search\"}]  # # Perform web search before answering the question  Use the Cohere web-search connector\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meta’s LLaMA 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "# Use torch.device for non-pipeline code\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the model Use GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ").to(device)  # Move model to device\n",
    "\n",
    "# Prepare inputs\n",
    "prompt = \"Translate English to French: Configuration files are easy to use!\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate model output\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "# Decode and print the output\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
