{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Set Proxy ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Proxy: http://127.0.0.1:7890\n",
      "HTTPS Proxy: http://127.0.0.1:7890\n",
      "All Proxy: socks5://127.0.0.1:7890\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 设置 HTTP 代理\n",
    "os.environ['http_proxy'] = \"http://127.0.0.1:7890\"\n",
    "os.environ['https_proxy'] = \"http://127.0.0.1:7890\"\n",
    "os.environ['all_proxy'] = \"socks5://127.0.0.1:7890\"\n",
    "\n",
    "# 确认代理已设置\n",
    "print(\"HTTP Proxy:\", os.environ.get('http_proxy'))\n",
    "print(\"HTTPS Proxy:\", os.environ.get('https_proxy'))\n",
    "print(\"All Proxy:\", os.environ.get('all_proxy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Inception\" is a science fiction film released in 2010, written and directed by Christopher Nolan. The film is known for its complex narrative structure and innovative visual effects, exploring themes of dreams, reality, and the subconscious.\n",
      "\n",
      "### Plot Summary:\n",
      "The story follows Dom Cobb (played by Leonardo DiCaprio), a skilled \"extractor\" who enters the dreams of others to steal secrets from their subconscious. Cobb is offered a chance to have his criminal history erased if he can successfully perform \"inception\"—the act of planting an idea into someone's mind without them realizing it. To accomplish this, he assembles a team, including Arthur (Joseph Gordon-Levitt), Ariadne (Elliot Page), Eames (Tom Hardy), and Yusuf (Dileep Rao). They delve into the layered dream worlds, facing various challenges and manifestations of Cobb's troubled past, particularly involving his deceased wife, Mal (Marion Cotillard).\n",
      "\n",
      "### Cast:\n",
      "- Leonardo DiCaprio as Dom Cobb\n",
      "- Joseph Gordon-Levitt as Arthur\n",
      "- Ellen Page as Ariadne\n",
      "- Tom Hardy as Eames\n",
      "- Ken Watanabe as Saito\n",
      "- Cillian Murphy as Robert Fischer\n",
      "- Marion Cotillard as Mal Cobb\n",
      "- Michael Caine as Professor Stephen Miles\n",
      "\n",
      "### Reception:\n",
      "\"Inception\" received critical acclaim for its screenplay, direction, visual effects, and performances, particularly DiCaprio's. It was a commercial success, grossing over $836 million worldwide. The film won four Academy Awards, including Best Cinematography, Best Visual Effects, Best Sound Editing, and Best Sound Mixing, and it was nominated for several others, including Best Picture and Best Original Screenplay.\n",
      "\n",
      "### Themes:\n",
      "The film explores various themes, such as the nature of reality, the subconscious mind, grief, and the complexities of human relationships. Its intricate plot structure and philosophical questions have led to extensive analysis and discussion among audiences and critics alike.\n",
      "\n",
      "### Legacy:\n",
      "\"Inception\" is often regarded as one of the best films of the 21st century and has influenced numerous other works in cinema. Its iconic imagery and memorable score by Hans Zimmer have further solidified its place in popular culture.\n",
      "\n",
      "If you have any specific questions or need more detailed information about \"Inception,\" feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# Create a chat model\n",
    "chat = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.5)\n",
    "\n",
    "# Create a system prompt\n",
    "template = \"You are an assistant that helps users find information about movies.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "# Create a human prompt\n",
    "human_template = \"Find information about the movie {movie_title}.\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "# Create a chat prompt\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# Chat with the model\n",
    "response = chat(chat_prompt.format_prompt(movie_title=\"Inception\").to_messages())\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarization Chain Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xixiw\\AppData\\Local\\Temp\\ipykernel_25768\\4078935071.py:18: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  summary = summarize_chain(document)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \"One Page Linux Manual\" provides a comprehensive overview of essential Linux commands, organized into categories such as starting and stopping the system, accessing and mounting file systems, finding files, managing files, installing software, user administration, and configuration files. Key commands include:\n",
      "\n",
      "- **System Management**: `shutdown`, `reboot`, `halt`, `startx`.\n",
      "- **File System Management**: `mount`, `umount`, `find`, `locate`, `grep`.\n",
      "- **File Operations**: `ls`, `cp`, `mv`, `rm`, `cat`, `more`, `head`, `tail`.\n",
      "- **Software Installation**: `rpm` commands for installing, upgrading, and removing packages, and `tar` for decompressing archives.\n",
      "- **User Management**: `adduser`, `passwd`, `su`, `exit`.\n",
      "- **Network and System Utilities**: `ifconfig`, `ps`, `kill`, `tail -f`, `cat /var/log/dmesg`.\n",
      "- **File Permissions**: `chmod` for modifying file access rights.\n",
      "- **Printing**: Commands for managing print jobs and configuring printers.\n",
      "\n",
      "The manual also includes tips, wildcards for file operations, and configuration file descriptions, making it a handy reference for Linux users.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Initialize language model\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Load the summarization chain\n",
    "summarize_chain = load_summarize_chain(llm)\n",
    "\n",
    "# Load the document using PyPDFLoader\n",
    "document_loader = PyPDFLoader(file_path=\"The_One_Page_Linux_Manual.pdf\")\n",
    "document = document_loader.load()\n",
    "\n",
    "# Summarize the document\n",
    "summary = summarize_chain(document)\n",
    "print(summary['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chain_type=\"map_reduce\" [default]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Installation\\Anaconda\\envs\\llmc\\lib\\site-packages\\langchain_community\\chat_models\\openai.py:173: UserWarning: Unexpected type for token usage: <class 'NoneType'>\n",
      "  warnings.warn(f\"Unexpected type for token usage: {type(new_usage)}\")\n",
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document \"THE ONE PAGE LINUX MANUAL\" is a comprehensive guide that provides a quick reference to essential Linux commands, organized into categories for ease of use. It covers system management tasks such as starting and stopping the system, accessing and mounting file systems, and finding files and text within files. The guide includes commands for configuring the X Window System, managing files, and installing software using RPM packages and tar archives. It also addresses user administration, offering commands for creating users, setting passwords, and switching to superuser mode. Additionally, the manual provides tips and tricks for network configuration, system monitoring, and file management using wildcards. It details important configuration files for system settings, file permissions, and printing management. The guide is particularly useful for Redhat users, offering specific shortcuts and commands for managing the X Window System and printing tasks. Overall, it serves as a valuable resource for performing common Linux tasks efficiently.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize language model\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Create custom map prompt template\n",
    "map_template = \"\"\"\n",
    "You are a helpful assistant that provides very detailed summaries. \n",
    "Please summarize the following document, highlighting the most important points:\n",
    "{text}\n",
    "\"\"\"\n",
    "map_prompt_template = PromptTemplate(input_variables=[\"text\"], template=map_template)\n",
    "\n",
    "# Create custom combine prompt template\n",
    "combine_template = \"\"\"\n",
    "You are a helpful assistant that provides very detailed summaries. \n",
    "Here are summaries of various parts of a document. Please combine them into a concise, informative summary:\n",
    "(please provide the answer in plain text without any special characters or formatting)\n",
    "{text}\n",
    "\"\"\"\n",
    "combine_prompt_template = PromptTemplate(input_variables=[\"text\"], template=combine_template)\n",
    "\n",
    "# Load the summarization chain with custom map and combine prompts\n",
    "summarize_chain = load_summarize_chain(llm, chain_type=\"map_reduce\", map_prompt=map_prompt_template, combine_prompt=combine_prompt_template)\n",
    "\n",
    "# Load the document using PyPDFLoader\n",
    "document_loader = PyPDFLoader(file_path=\"The_One_Page_Linux_Manual.pdf\")\n",
    "documents = document_loader.load()  # Make sure it loads the document content correctly\n",
    "\n",
    "# Summarize the document using the custom prompt\n",
    "summary = summarize_chain.run(input_documents=documents)\n",
    "\n",
    "# Output the result\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chain_type=\"refine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document provides a comprehensive overview of various Linux commands, configuration files, file permissions, shortcuts, and printing commands. \n",
      "\n",
      "Key commands include:\n",
      "\n",
      "- e2fsck for checking filesystems on partitions.\n",
      "- fdformat for formatting floppy disks.\n",
      "- tar for backing up directory contents to floppy disks.\n",
      "- tail and cat for displaying system logs and boot messages.\n",
      "- Wildcard usage in commands for file manipulation, including the asterisk for all files, question mark for single character matches, and brackets for character choices.\n",
      "\n",
      "It also covers entering single user mode for password recovery and managing processes with ps and kill commands.\n",
      "\n",
      "Important configuration files are detailed, such as:\n",
      "\n",
      "- /etc/profile for system-wide environment variables.\n",
      "- /etc/fstab for device mount points.\n",
      "- /etc/motd for login messages.\n",
      "- /etc/hosts for hostname and IP address mappings.\n",
      "- /etc/httpd/conf for Apache server parameters.\n",
      "- /etc/smb.conf for SAMBA server configuration.\n",
      "\n",
      "File permissions are explained, including the use of chmod to modify permissions using octal codes. The document outlines the meaning of read, write, and execute permissions.\n",
      "\n",
      "Shortcuts for graphical environments, particularly in Redhat, are listed, such as adjusting screen resolution and managing windows.\n",
      "\n",
      "Printing commands are also included, detailing how to start and stop the print daemon, manage print jobs, and print manual pages.\n",
      "\n",
      "Overall, the document serves as a practical guide for Linux users, providing essential commands and configurations for system management and operation.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize language model\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Create a custom prompt template for summarization\n",
    "template = \"\"\"\n",
    "You are a helpful assistant that provides very detailed summaries. \n",
    "Please summarize the following document, highlighting the most important points in a concise and informative manner:\n",
    "(\"Do not use any special characters or formatting such as *, -, _, or bullet points. \")\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# Initialize the prompt template\n",
    "prompt_template = PromptTemplate(input_variables=[\"text\"], template=template)\n",
    "\n",
    "# Use 'refine' chain type, which supports custom prompts\n",
    "summarize_chain = load_summarize_chain(llm, chain_type=\"refine\", refine_prompt=prompt_template)\n",
    "\n",
    "# Load the document using PyPDFLoader\n",
    "document_loader = PyPDFLoader(file_path=\"The_One_Page_Linux_Manual.pdf\")\n",
    "documents = document_loader.load()  # Ensure document content is correctly loaded\n",
    "\n",
    "# Summarize the document using the custom prompt\n",
    "summary = summarize_chain.run(input_documents=documents)\n",
    "\n",
    "# Output the result\n",
    "print(summary)\n",
    "\n",
    "# (\"The answer should be a continuous paragraph with no symbols.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QA Chain Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what is the meaning of life?\n",
      "AI-generated answer: The meaning of life is a philosophical question that has been explored by many cultures and thinkers throughout history. It can vary greatly from person to person, often encompassing themes of purpose, fulfillment, relationships, and personal growth. Some find meaning through connections with others, while others seek it through achievements, experiences, or spiritual beliefs. Ultimately, it is a subjective concept that each individual must define for themselves.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "# Define the template for the song title prompt\n",
    "template = \"\"\"Question: {question}\\nAnswer(please provide the answer in plain text without any special characters or formatting):\"\"\"\n",
    "\n",
    "# Create a PromptTemplate with the question as input\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Create the LLMChain with the specified prompt and model\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Input data for the prompt, with theme and year specified\n",
    "input_data = {\"question\": \"what is the meaning of life?\"}\n",
    "\n",
    "# Run the chain to get the answer for the question\n",
    "response = chain.run(input_data)\n",
    "\n",
    "# Output the question and AI-generated answer\n",
    "print(\"Question: what is the meaning of life?\")\n",
    "print(\"AI-generated answer:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Connectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q llama-index==0.9.14.post3 deeplake==3.8.8 numpy==1.24.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "# Use download_loader to load the WikipediaReader from LlamaHub\n",
    "WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "\n",
    "# Initialize an instance of WikipediaReader\n",
    "loader = WikipediaReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from specified Wikipedia pages and store them as Document objects\n",
    "documents = loader.load_data(pages=['Natural Language Processing', 'Artificial Intelligence'])\n",
    "\n",
    "# Print the number of documents loaded\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "from llama_index.node_parser import SimpleNodeParser\n",
    "\n",
    "# Initialize the parser\n",
    "parser = SimpleNodeParser.from_defaults(chunk_size=512, chunk_overlap=20)\n",
    "\n",
    "# Parse documents into nodes\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "print(len(nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Store Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://yangalex/LlamaIndex_intro already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "from llama_index.vector_stores import DeepLakeVectorStore\n",
    "\n",
    "my_activeloop_org_id = \"yangalex\"\n",
    "my_activeloop_dataset_name = \"LlamaIndex_intro\"\n",
    "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
    "\n",
    "# Create an index over the documnts\n",
    "vector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading data to deeplake dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:39<00:00,  1.27s/it]\n",
      "|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://yangalex/LlamaIndex_intro', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape      dtype  compression\n",
      "  -------    -------    -------    -------  ------- \n",
      " embedding  embedding  (62, 1536)  float32   None   \n",
      "    id        text      (62, 1)      str     None   \n",
      " metadata     json      (62, 1)      str     None   \n",
      "   text       text      (62, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    }
   ],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Lake Query Engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP stands for Natural Language Processing.\n"
     ]
    }
   ],
   "source": [
    "# Set up a query engine using the created index\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Query the index with a question\n",
    "response = query_engine.query(\"What does NLP stands for?\")\n",
    "\n",
    "# Print the response\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT Vector Store Query Engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP stands for Natural Language Processing.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import GPTVectorStoreIndex\n",
    "\n",
    "# Create an index from a collection of documents\n",
    "index = GPTVectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# Set up a query engine using the created index\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Query the index with a question\n",
    "response = query_engine.query(\"What does NLP stands for?\")\n",
    "\n",
    "# Print the response\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store index as vector embeddings on the disk\n",
    "index.storage_context.persist()\n",
    "# This saves the data in the 'storage' by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index from storage...\n"
     ]
    }
   ],
   "source": [
    "# Index Storage Checks\n",
    "import os.path\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index import download_loader\n",
    "\n",
    "# Let's see if our index already exists in storage.\n",
    "if not os.path.exists(\"./storage\"):\n",
    "    print(\"Loading Wikipedia data...\")\n",
    "    # If not, we'll load the Wikipedia data and create a new index\n",
    "    WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "    loader = WikipediaReader()\n",
    "    documents = loader.load_data(pages=['Natural Language Processing', 'Artificial Intelligence'])\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # Index storing\n",
    "    index.storage_context.persist()\n",
    "else:\n",
    "    # If the index already exists, we'll just load it:\n",
    "    print(\"Loading index from storage...\")\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "    index = load_index_from_storage(storage_context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
